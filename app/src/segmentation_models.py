# -*- coding: utf-8 -*-
"""Segmentation Models

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14rmCW7loEn6FxpXXHcSE-cDGym99kB4f

# Introduction
This notebook demonstrates a few models for image segmentation, for use with the image inpainting models demonstrated in the second phase of this project.

Given a cropped portion of an image by the user, the models' goal is to segment out an object within the area. This mask will then be used to mask away the object, for the inpainter to fill in the remaining "hole".

Shoutout to [this notebook](https://www.kaggle.com/balraj98/unet-for-building-segmentation-pytorch/data) for a significant portion of the code used here.

## Importing modules
"""

# Commented out IPython magic to ensure Python compatibility.
import warnings
warnings.filterwarnings("ignore")
import numpy as np
import torch
import torch.nn as nn
import pycocotools
from pycocotools.coco import COCO
import torchvision.models as models
import os
from PIL import Image, ImageOps
from matplotlib import pyplot as plt
# %matplotlib inline
import cv2
from tqdm import tqdm
import matplotlib.patches as patches
import albumentations as album
import random
import segmentation_models_pytorch as smp
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
from torchvision import transforms
from segmentation_models_pytorch.encoders import get_preprocessing_fn
from torchinfo import summary

# For reproducible results
seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

NUM_WORKERS = os.cpu_count()
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f'Using device {DEVICE}')
print(f'Total available CPUs: {NUM_WORKERS}')

"""## Dataset processing"""

# This dataset is a pre-processed version of a subset of the COCO dataset, where each image is cropped using the bounding box of its most significant object, and a binary mask created based on said object's mask.
# View this notebook for the code used to output this dataset: https://colab.research.google.com/drive/1XDYp4i09V2gHcQV7xCdjDBcdvCbtpqJs?usp=sharing
srcpath = os.path.dirname(__file__)
apppath = os.path.dirname(srcpath)

# root_dataset_dir = '/kaggle/input/coco-bin-masks/coco_bin_masks'
# os.listdir(root_dataset_dir)

# x_train_dir = os.path.join(root_dataset_dir, 'train', 'images')
# y_train_dir = os.path.join(root_dataset_dir, 'train', 'masks')
# x_valid_dir = os.path.join(root_dataset_dir, 'valid', 'images')
# y_valid_dir = os.path.join(root_dataset_dir, 'valid', 'masks')
# x_test_dir = os.path.join(root_dataset_dir, 'test', 'images')
# y_test_dir = os.path.join(root_dataset_dir, 'test', 'masks')

import torch
import cv2
import numpy as np
import os
from glob import glob

# class COCODataset(Dataset):
#     def __init__(
#             self,
#             images_dir,
#             masks_dir,
#             augmentation=None,
#             preprocessing=None,
#     ):
#         self.images_dir = images_dir
#         self.masks_dir = masks_dir
#         self.augmentation = augmentation
#         self.preprocessing = preprocessing

#         # Get names for images and masks in respective directory

#         self.image_paths = sorted(glob(os.path.join(images_dir, '*')))
#         image_basenames = [os.path.splitext(os.path.basename(p))[0] for p in self.image_paths]

#         mask_files = glob(os.path.join(masks_dir, '*'))
#         mask_dict = {os.path.splitext(os.path.basename(p))[0]: p for p in mask_files}

#         self.mask_paths = []
#         matched_image_paths = []
#         for basename, img_path in zip(image_basenames, self.image_paths): # Match images with their respective masks
#             mask_path = mask_dict.get(basename)
#             if mask_path is not None:
#                 matched_image_paths.append(img_path)
#                 self.mask_paths.append(mask_path)
#             else:
#                 print(f"Warning: No mask found for image: {img_path}")

#         self.image_paths = matched_image_paths  # only keep images that have masks

#         assert len(self.image_paths) == len(self.mask_paths), "Mismatch between images and masks!"

#     def __getitem__(self, i):
#         image = cv2.imread(self.image_paths[i])
#         if image is None:
#             raise FileNotFoundError(f"Image not found: {self.image_paths[i]}")
#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

#         mask = cv2.imread(self.mask_paths[i], cv2.IMREAD_GRAYSCALE)
#         if mask is None:
#             raise FileNotFoundError(f"Mask not found: {self.mask_paths[i]}")

#         mask = (mask > 127).astype('float32') # Create binary mask

#         if self.augmentation:
#             sample = self.augmentation(image=image, mask=mask)
#             image, mask = sample['image'], sample['mask']
#         if self.preprocessing:
#             sample = self.preprocessing(image=image, mask=mask)
#             image, mask = sample['image'], sample['mask']

#         if mask.ndim == 2:
#             mask = np.expand_dims(mask, axis=0)

#         return image, mask

#     def __len__(self):
#         return len(self.image_paths)

# def to_tensor(x, **kwargs):
#     if x.ndim == 2:  # mask (H,W)
#         x = np.expand_dims(x, axis=0)
#     elif x.ndim == 3:  # image (H,W,C)
#         x = x.transpose(2, 0, 1)
#     return x.astype('float32')
#     raise ValueError(f"Unsupported ndim in to_tensor: {x.ndim}")
# def get_train_augmentations():
#   train_transforms = [
#     album.HorizontalFlip(p=0.5),
#     album.RandomRotate90(p=0.5),
#     album.Resize(height=256, width=256),
#   ]
#   return album.Compose(train_transforms)
# def get_valid_augmentations():
#   valid_transforms = [
#     album.Resize(height=256, width=256),
#   ]
#   return album.Compose(valid_transforms)
# def get_preprocessing(preprocessing_fn=None):
#     _transform = []
#     if preprocessing_fn:
#         _transform.append(album.Lambda(image=preprocessing_fn))
#     _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))

#     return album.Compose(_transform)

# dataset = COCODataset(
#     x_train_dir, y_train_dir, augmentation=get_train_augmentations()
# )
# image, mask = dataset[3]
# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))
# ax1.imshow(image)
# ax2.imshow(mask[0], cmap='gray')
# plt.show()

# vgg16_fn = get_preprocessing_fn('vgg16', pretrained='imagenet') # Get preprocessing function based on the backbone

# train_dataset = COCODataset(x_train_dir, y_train_dir, get_train_augmentations(), get_preprocessing(preprocessing_fn=vgg16_fn))
# valid_dataset = COCODataset(x_valid_dir, y_valid_dir, get_valid_augmentations(), get_preprocessing(preprocessing_fn=vgg16_fn))

# train_loader = DataLoader(train_dataset, batch_size=16, shuffle = True, num_workers=NUM_WORKERS, persistent_workers=False)
# valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle = False, num_workers=NUM_WORKERS, persistent_workers=False)

"""## Defining models

FCN16s and UNet both uses a pre-trained VGG16 backbone for their encoding stages.

### FCN16s
"""

class FCN16s(nn.Module):
    def __init__(self, pretrained_net, n_class):
        super().__init__()
        self.n_class = n_class
        self.pretrained_net = pretrained_net
        self.relu    = nn.ReLU(inplace=True)
        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)
        self.bn1     = nn.BatchNorm2d(512)
        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)
        self.bn2     = nn.BatchNorm2d(256)
        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)
        self.bn3     = nn.BatchNorm2d(128)
        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)
        self.bn4     = nn.BatchNorm2d(64)
        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)
        self.bn5     = nn.BatchNorm2d(32)
        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)

    def forward(self, x):
        output = self.pretrained_net(x)
        x5 = output['x5']  # size=(N, 512, x.H/32, x.W/32)
        x4 = output['x4']  # size=(N, 512, x.H/16, x.W/16)

        score = self.relu(self.deconv1(x5))               # size=(N, 512, x.H/16, x.W/16)
        score = self.bn1(score + x4)                      # element-wise add, size=(N, 512, x.H/16, x.W/16)
        score = self.bn2(self.relu(self.deconv2(score)))  # size=(N, 256, x.H/8, x.W/8)
        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)
        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)
        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)
        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)

        return F.interpolate(input=score, scale_factor=0.5, mode='bilinear', align_corners=True)  # size=(N, n_class, x.H/2, x.W/2)
class VGG16Backbone(nn.Module):
    def __init__(self):
        super().__init__()
        vgg = models.vgg16_bn(pretrained=True)
        features = list(vgg.features.children())
        self.stage1 = nn.Sequential(*features[:33])  # up to pool4
        self.stage2 = nn.Sequential(*features[33:43])  # pool5

    def forward(self, x):
        x = self.stage1(x)
        x4 = x  # after pool4, stride 16
        x = self.stage2(x)
        x5 = x  # after pool5, stride 32
        return {'x4': x4, 'x5': x5}
backbone = VGG16Backbone()

class DoubleConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        return self.conv(x)

class DecoderBlock(nn.Module):
    def __init__(self, in_channels, skip_channels, out_channels):
        super().__init__()
        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)
        self.conv = DoubleConv(out_channels + skip_channels, out_channels)

    def forward(self, x, skip):
        x = self.up(x)
        if x.size()[2:] != skip.size()[2:]:  # Handle spatial misalignment
            x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=True)
        x = torch.cat([x, skip], dim=1)
        return self.conv(x)

class UNet(nn.Module):
    def __init__(self, num_classes=1):
        super().__init__()

        vgg16 = models.vgg16_bn(pretrained=True)
        features = vgg16.features  # Extract VGG's convolutional layers (backbone)

        # Encoder layers from VGG-16
        self.encoder1 = nn.Sequential(*features[:6])    # Conv1, Relu1, MaxPool1
        self.encoder2 = nn.Sequential(*features[6:13])  # Conv2, Relu2, MaxPool2
        self.encoder3 = nn.Sequential(*features[13:23]) # Conv3, Relu3, MaxPool3
        self.encoder4 = nn.Sequential(*features[23:33]) # Conv4, Relu4, MaxPool4
        self.encoder5 = nn.Sequential(*features[33:43]) # Conv5, Relu5, MaxPool5

        # Bridge (Bottleneck Layer)
        self.bridge = DoubleConv(512, 512)

        # Decoder layers
        self.up1 = DecoderBlock(512, 512, 256)  # From bottleneck and encoder4
        self.up2 = DecoderBlock(256, 256, 128)  # From up1 and encoder3
        self.up3 = DecoderBlock(128, 128, 64)   # From up2 and encoder2
        self.up4 = DecoderBlock(64, 64, 32)     # From up3 and encoder1

        # Final segmentation head
        self.final = nn.Sequential(
            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, num_classes, kernel_size=1)
        )

    def forward(self, x):
        # Encoder
        e1 = self.encoder1(x)   # Stride: 2
        e2 = self.encoder2(e1)  # Stride: 4
        e3 = self.encoder3(e2)  # Stride: 8
        e4 = self.encoder4(e3)  # Stride: 16
        e5 = self.encoder5(e4)  # Stride: 32

        # Bottleneck
        b = self.bridge(e5)

        # Decoder
        d1 = self.up1(b, e4)  # Up from 32x downsampled to 16x
        d2 = self.up2(d1, e3)  # Up from 16x to 8x
        d3 = self.up3(d2, e2)  # Up from 8x to 4x
        d4 = self.up4(d3, e1)  # Up from 4x to 2x

        # Final output
        return self.final(d4)

"""## Training models"""

# Move instance of models to device
fcn16s = FCN16s(backbone, n_class=1).to(DEVICE)
unet = UNet().to(DEVICE)
summary(fcn16s, (16,3,256,256))

summary(unet, (16,3,256,256))

EPOCHS = 20
PATIENCE = 5

train_logs_fcn16s = []
valid_logs_fcn16s = []
train_logs_unet = []
valid_logs_unet = []

def smp_metrics(output, gt, metric):
    assert (metric in ['f1', 'iou', 'recall', 'acc'])
    pred = (torch.sigmoid(output) > 0.5).long() # map logits to probabilities for metrics calculation
    gt = (gt > 0.5).long() # thresholding
    tp, fp, fn, tn = smp.metrics.get_stats(pred, gt, mode='binary')
    match metric:
        case 'f1':
            return smp.metrics.f1_score(tp,fp,fn,tn, reduction='macro')
        case 'iou':
            return smp.metrics.iou_score(tp,fp,fn,tn, reduction='macro')
        case 'acc':
            return smp.metrics.accuracy(tp,fp,fn,tn, reduction='macro')
        case 'recall':
            return smp.metrics.recall(tp,fp,fn,tn, reduction='macro')


def train_one_epoch(model, dataloader, optimizer, loss_fn, device):
    model.train()
    epoch_loss = 0.0
    losses, ious, accs, f1s, recalls = [], [], [], [], []
    loop = tqdm(dataloader, desc = 'Training', leave=False)
    for images, masks in loop:
        images = images.to(device, dtype=torch.float32)
        masks  = masks.to(device, dtype=torch.float32)
        outputs = model(images)

        loss = loss_fn(outputs, masks)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        iou = smp_metrics(outputs, masks, 'iou').item()
        acc = smp_metrics(outputs, masks, 'acc').item()
        f1 = smp_metrics(outputs, masks, 'f1').item()
        recall = smp_metrics(outputs, masks, 'recall').item()

        losses.append(loss.item())
        ious.append(iou)
        accs.append(acc)
        f1s.append(f1)
        recalls.append(recall)

        epoch_loss += loss.item() * images.size(0)
        loop.set_postfix(loss=np.mean(losses), acc=np.mean(accs), f1=np.mean(f1s), iou=np.mean(ious), recall=np.mean(recalls))

    return {
        "loss": epoch_loss / len(dataloader.dataset),
        "accuracy": np.mean(accs),
        "iou": np.mean(ious),
        "f1": np.mean(f1s),
        "recall" : np.mean(recalls)
    }

def eval_one_epoch(model, dataloader, loss_fn, device):
    model.eval()
    val_loss = 0.0
    losses, ious, accs, f1s, recalls = [], [], [], [], []
    with torch.no_grad():
        loop = tqdm(dataloader, desc="Validation", leave=False)
        for images, masks in loop:
            images = images.to(device, dtype=torch.float32)
            masks  = masks.to(device, dtype=torch.float32)

            outputs = model(images)
            loss = loss_fn(outputs, masks)

            iou = smp_metrics(outputs, masks, 'iou').item()
            acc = smp_metrics(outputs, masks, 'acc').item()
            f1 = smp_metrics(outputs, masks, 'f1').item()
            recall = smp_metrics(outputs, masks, 'recall').item()

            losses.append(loss.item())
            ious.append(iou)
            accs.append(acc)
            f1s.append(f1)
            recalls.append(recall)
            val_loss += loss.item() * images.size(0)
            loop.set_postfix(loss=np.mean(losses), acc=np.mean(accs), f1=np.mean(f1s), iou=np.mean(ious), recall=np.mean(recalls))

    return {
        "loss": val_loss / len(dataloader.dataset),
        "accuracy": np.mean(accs),
        "iou": np.mean(ious),
        "f1": np.mean(f1s),
        "recall" : np.mean(recalls)
    }

loss = nn.BCEWithLogitsLoss()

training_fcn16s = False
training_unet = False

"""### FCN16s"""

if training_fcn16s == True:
    optimizer = torch.optim.Adam([
        dict(params=fcn16s.parameters(), lr=1e-4, weight_decay=0.0001),
    ])

    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=5, T_mult=1, eta_min=5e-5,
    )

    checkpoint_path = '/kaggle/input/best-fcn16s2/pytorch/default/1/best_fcn16s.pth'

    if os.path.exists(checkpoint_path):
        checkpoint = torch.load(checkpoint_path,weights_only=False, map_location=DEVICE)
        fcn16s.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        best_iou = checkpoint.get('best_iou', 0.0)
        train_logs_fcn16s = checkpoint.get('train_logs', [])
        valid_logs_fcn16s = checkpoint.get('valid_logs', [])
        print(f'Checkpoint loaded. Previously ended at epoch {start_epoch}, with mIoU = {best_iou}')

    else:
        print('No checkpoint detected. Model will retrain from the beginning.')

        start_epoch = 0
        best_iou = 0.0
    print('Now training FCN16s')
    current_no_improve = 0

    for epoch in range(start_epoch, EPOCHS):

        print(f"\nEpoch {epoch+1}/{EPOCHS} | lr = {optimizer.param_groups[0]["lr"]:.10f}")

        train_metrics = train_one_epoch(fcn16s, train_loader, optimizer, loss, DEVICE)
        print(f"Train loss: {train_metrics['loss']:.4f}, acc: {train_metrics['accuracy']:.4f}, f1: {train_metrics['f1']:.4f}, mIoU: {train_metrics['iou']:.4f}, Recall: {train_metrics['recall']:.4f}")

        val_metrics = eval_one_epoch(fcn16s, valid_loader, loss, DEVICE)
        print(f"Val loss: {val_metrics['loss']:.4f}, acc: {val_metrics['accuracy']:.4f}, f1: {val_metrics['f1']:.4f}, mIoU: {val_metrics['iou']:.4f}, Recall: {val_metrics['recall']:.4f}")

        train_logs_fcn16s.append([train_metrics['loss'], train_metrics['accuracy'], train_metrics['f1'], train_metrics['iou'], train_metrics['recall']])
        valid_logs_fcn16s.append([val_metrics['loss'], val_metrics['accuracy'], val_metrics['f1'], val_metrics['iou'], val_metrics['recall']])

        lr_scheduler.step()

        # Save model if validation mIoU improves
        if val_metrics['iou'] > best_iou:
            torch.save({
                'epoch': epoch,
                'model_state_dict': fcn16s.state_dict(),
                'train_logs': train_logs_fcn16s,
                'valid_logs': valid_logs_fcn16s,
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': lr_scheduler.state_dict(),
                'best_iou': best_iou
            }, 'best_fcn16s.pth')
            best_iou = val_metrics['iou']
            print(f"New best model saved (val mIoU: {best_iou:.4f})")
            current_no_improve = 0
        else:
            current_no_improve += 1
            if current_no_improve >= PATIENCE:
                print(f'Early stopping on epoch {epoch + 1}. Patience has run out.')
                break

"""### UNet"""

if training_unet == True:
    optimizer = torch.optim.Adam([
        dict(params=unet.parameters(), lr=1e-4, weight_decay=0.0001),
    ])
    # define learning rate scheduler
    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=5, T_mult=1, eta_min=5e-5,
    )
    checkpoint_path = '/kaggle/input/best-unet/pytorch/default/1/best_unet.pth'
    if os.path.exists(checkpoint_path):
        checkpoint = torch.load(checkpoint_path,weights_only=False, map_location=DEVICE)
        unet.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        best_iou = checkpoint.get('best_iou', 0.0)
        train_logs_unet = checkpoint.get('train_logs', [])
        valid_logs_unet = checkpoint.get('valid_logs', [])
        print(f'Checkpoint loaded. Previously ended at epoch {start_epoch}, with mIoU = {best_iou}')
    else:
        print('No checkpoint detected. Model will retrain from the beginning.')
        start_epoch = 0
        best_iou = 0.0

    print('Now training UNet')
    current_no_improve = 0

    for epoch in range(start_epoch, EPOCHS):

        print(f"\nEpoch {epoch+1}/{EPOCHS} | lr = {optimizer.param_groups[0]["lr"]:.10f}")

        train_metrics = train_one_epoch(unet, train_loader, optimizer, loss, DEVICE)
        print(f"Train loss: {train_metrics['loss']:.4f}, acc: {train_metrics['accuracy']:.4f}, f1: {train_metrics['f1']:.4f}, mIoU: {train_metrics['iou']:.4f}, Recall: {train_metrics['recall']:.4f}")

        val_metrics = eval_one_epoch(unet, valid_loader, loss, DEVICE)
        print(f"Val loss: {val_metrics['loss']:.4f}, acc: {val_metrics['accuracy']:.4f}, f1: {val_metrics['f1']:.4f}, mIoU: {val_metrics['iou']:.4f}, Recall: {val_metrics['recall']:.4f}")

        train_logs_unet.append([train_metrics['loss'], train_metrics['accuracy'], train_metrics['f1'], train_metrics['iou'], train_metrics['recall']])
        valid_logs_unet.append([val_metrics['loss'], val_metrics['accuracy'], val_metrics['f1'], val_metrics['iou'], val_metrics['recall']])

        lr_scheduler.step()

        # Save model if validation mIoU improves
        if val_metrics['iou'] > best_iou:
            torch.save({
                'epoch': epoch,
                'model_state_dict': unet.state_dict(),
                'train_logs': train_logs_unet,
                'valid_logs': valid_logs_unet,
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': lr_scheduler.state_dict(),
                'best_iou': best_iou
            }, 'best_unet.pth')
            best_iou = val_metrics['iou']
            print(f"New best model saved (val mIoU: {best_iou:.4f})")
            current_no_improve = 0
        else:
            current_no_improve += 1
            if current_no_improve >= PATIENCE:
                print(f'Early stopping on epoch {i + 1}. Patience has run out.')
                break

"""## Visualizing training metrics throughout"""

# def stats_plotter(train_logs, valid_logs):
#     tick_interval= 0.05
#     train_loss = [x[0] for x in train_logs]
#     val_loss = [x[0] for x in valid_logs]
#     train_acc = [x[1] for x in train_logs]
#     val_acc = [x[1] for x in valid_logs]
#     train_f1 = [x[2] for x in train_logs]
#     val_f1 = [x[2] for x in valid_logs]
#     train_iou = [x[3] for x in train_logs]
#     val_iou = [x[3] for x in valid_logs]
#     train_recall = [x[4] for x in train_logs]
#     val_recall = [x[4] for x in valid_logs]

#     epochs = np.arange(1, len(train_loss)+1)

#     fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5, 1, figsize=(12, 25))

#     # Plot Loss
#     ax1.plot(epochs, train_loss, marker='o', label='Train Loss')
#     ax1.plot(epochs, val_loss, marker='x', label='Validation Loss')
#     ax1.set_xlabel('Epoch')
#     ax1.set_ylabel('BCE Loss')
#     ax1.set_ylim(0, max(train_loss + val_loss) + 0.1)
#     ax1.set_title('Binary Cross-Entropy Loss per Epoch')
#     ax1.legend()
#     ax1.grid(True)
#     ax1.set_xticks(epochs[::2])

#     # Plot Accuracy
#     ax2.plot(epochs, train_acc, marker='o', label='Train Accuracy')
#     ax2.plot(epochs, val_acc, marker='x', label='Validation Accuracy')
#     ax2.set_xlabel('Epoch')
#     ax2.set_ylabel('Accuracy')
#     ax2.set_ylim(min(train_acc + val_acc) - 0.1,1)
#     ax2.set_title('Accuracy per Epoch')
#     ax2.legend()
#     ax2.grid(True)
#     ax2.set_xticks(epochs[::2])


#     # Plot F1
#     ax3.plot(epochs, train_f1, marker='o', label='Train F1')
#     ax3.plot(epochs, val_f1, marker='x', label='Validation F1')
#     ax3.set_xlabel('Epoch')
#     ax3.set_ylabel('F1 Score')
#     ax3.set_ylim(min(train_f1 + val_f1) - 0.1,1)
#     ax3.set_title('F1 Score per Epoch')
#     ax3.legend()
#     ax3.grid(True)
#     ax3.set_xticks(epochs[::2])

#     # Plot mIOU
#     ax4.plot(epochs, train_iou, marker='o', label='Train mIOU')
#     ax4.plot(epochs, val_iou, marker='x', label='Validation mIOU')
#     ax4.set_xlabel('Epoch')
#     ax4.set_ylabel('mIOU')
#     ax4.set_ylim(min(train_iou + val_iou) - 0.1,1)
#     ax4.set_title('mIOU per Epoch')
#     ax4.legend()
#     ax4.grid(True)
#     ax4.set_xticks(epochs[::2])

#     # Plot Recall
#     ax5.plot(epochs, train_recall, marker='o', label='Train Recall')
#     ax5.plot(epochs, val_recall, marker='x', label='Validation Recall')
#     ax5.set_xlabel('Epoch')
#     ax5.set_ylabel('Recall')
#     ax5.set_ylim(min(train_recall + val_recall) - 0.1,1)
#     ax5.set_title('Recall (TPR) per Epoch')
#     ax5.legend()
#     ax5.grid(True)
#     ax5.set_xticks(epochs[::2])

#     plt.tight_layout()
#     plt.show()

# stats_plotter(train_logs_fcn16s, valid_logs_fcn16s)
# stats_plotter(train_logs_unet, valid_logs_unet)

"""## Testing model performance"""

# os.makedirs('./sample_predictions', exist_ok=True)

# test_dataset = COCODataset(x_test_dir, y_test_dir, get_valid_augmentations(), get_preprocessing(preprocessing_fn=vgg16_fn))

# test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)

# test_dataset_vis = COCODataset(x_test_dir, y_test_dir, get_valid_augmentations())

def plot_sample_vis(image_vis, gt_mask, pred_mask, idx, model_name):
    file_name = f'{model_name}_{idx}.jpg'
    gt_mask = gt_mask.squeeze()
    pred_mask = pred_mask.squeeze()
    fig, axs = plt.subplots(1, 3, figsize=(12, 4))
    axs[0].imshow(image_vis)
    axs[0].set_title('Image')
    axs[0].axis('off')
    axs[1].imshow(gt_mask, cmap='gray')
    axs[1].set_title('Ground Truth')
    axs[1].axis('off')
    axs[2].imshow(pred_mask, cmap='gray')
    axs[2].set_title('Prediction')
    axs[2].axis('off')
    fig.suptitle(f"Sample {idx}")
    plt.tight_layout()
    plt.savefig(os.path.join('./sample_predictions',file_name))
    plt.show()

def visualize_predictions(model, test_dataset_vis, test_dataset, model_name, plot=False):
    model.eval()
    for idx in range(20):
        image_vis, mask_vis = test_dataset_vis[idx]
        image_input = test_dataset[idx][0]
        image_tensor = torch.from_numpy(image_input).unsqueeze(0).to(DEVICE)
        with torch.no_grad():
            logits = model(image_tensor)
            pred_mask = (torch.sigmoid(logits) > 0.5).float().cpu().numpy()[0,0]
        if plot:
            plot_sample_vis(image_vis, mask_vis, pred_mask, idx, model_name)
        return pred_mask

def test_model(model, test_loader, loss, device):
    test_metrics = eval_one_epoch(model, test_loader, loss, device)
    print(f"Test loss: {test_metrics['loss']:.4f}, acc: {test_metrics['accuracy']:.4f}, f1: {test_metrics['f1']:.4f}, mIoU: {test_metrics['iou']:.4f}, Recall: {test_metrics['recall']:.4f}")

def run_inference_from_image(
    image: Image.Image | str,
    model_path: str,
    model_type: str,
    highlight: tuple,
    plot=False):
    '''
    Parameters:
        model_type: 'fcn16s' or 'unet'
        highlight: (topX, topY, width, height)
    '''

    assert model_type in ['fcn16s', 'unet']
    device = torch.device('cpu')

    if model_type == 'fcn16s':
        model = FCN16s(backbone, n_class=1).to(device)
    elif model_type == 'unet':
        model = UNet().to(device)
    checkpoint = torch.load(model_path, weights_only=False, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])

    if isinstance(image, str):
        original_image = Image.open(image).convert('RGB')
    else:
        original_image = image
    topX, topY, width, height = highlight

    cropped_image = original_image.crop((topX, topY, topX + width, topY + height))

    preprocessing = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    image_tensor = preprocessing(cropped_image).unsqueeze(0).to(device)

    with torch.no_grad():
        output = model(image_tensor)

    mask = (torch.sigmoid(output) > 0.5).float().cpu().numpy()[0,0]

    mask_resized = np.array(Image.fromarray(mask).resize((width, height), Image.NEAREST))

    full_mask = np.zeros((original_image.height, original_image.width), dtype=np.float32)

    # Place the resized cropped mask into the corresponding area in the full mask
    full_mask[topY:topY + height, topX:topX + width] = mask_resized
    full_mask_pil = Image.fromarray(((full_mask) * 255).astype(np.uint8))

    if plot:
        fig, axes = plt.subplots(1, 2, figsize=(10, 5))

        axes[0].imshow(original_image)
        axes[0].set_title("Original Image")
        axes[0].axis("off")
        axes[1].imshow(full_mask, cmap="gray")
        axes[1].set_title("Predicted Mask")
        axes[1].axis("off")
        plt.tight_layout()
        plt.show()

    return full_mask_pil

if __name__ == "__main__":
    from coco_data import getRandomUrl
    from images import getImageFromUrl

    # load best saved models from this run
    modelpath = os.path.join(apppath, 'assets', 'seg_models')
    fcn16spath = os.path.join(modelpath, "best_fcn16s.pth")
    unetpath = os.path.join(modelpath, "best_unet.pth")

    # if os.path.exists(fcn16spath):
    #     best_fcn16s = torch.load(fcn16spath, weights_only=False)
    # if os.path.exists(unetpath):
    #     best_unet = torch.load(unetpath, weights_only=False)

    img = getImageFromUrl(getRandomUrl(samesize=True))

    highlight = (100, 100, 150, 150)
    fcn16s_img = run_inference_from_image(img, fcn16spath, 'fcn16s', highlight, plot=True)
    unet_img = run_inference_from_image(img, unetpath, 'unet', highlight, plot=True)

    # img.show(title="original")
    # fcn16s_img.show(title="fcn16s")
    # unet_img.show(title="unet")

    # metrics
    # test_model(best_fcn16s, test_loader, loss, DEVICE)
    # test_model(best_unet, test_loader, loss, DEVICE)