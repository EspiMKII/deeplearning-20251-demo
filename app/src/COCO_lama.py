# -*- coding: utf-8 -*-
"""COCO_Lama.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NXxFvppuLRPwR6EySCXPzYANWCUHC3TO

# Top

This notebook aims to provide a model that can inpaint large holes, which are the output of the previous segmentation model. A binary mask is passed along side the image to determine the holes: 1 if it is in hole area (the model will inpaint this part), 0 otherwise (the model will keep the original pixels)

**NOTE**: Please follow the below steps before running this notebook:
1. Download `lightning_logs.zip` from this [OneDrive](https://husteduvn-my.sharepoint.com/:f:/g/personal/chau_nb235481_sis_hust_edu_vn/IgCcACqdgg1yTpt4QKat28i-ATYnF3Vycf3NKqdayVVgs_w?e=c9vB62).
2. Run the below block.
3. If you want to train the model, remove all `#` in `Training process` section and run it.
4. For implementation, focus on `Evaluation` section. There are 2 way to print the inpainted image:
- `run_inference_from_folder`:
    - Require `image_folder_path`.
    - Automatically create mask and inpaint some images in the folder.
    - Returns torch tensors ($B\times C\times H\times W$) masked_images, inpainted_images, images
- `run_inference_from_image`:
    - Require `image` (can be a path to an image, numpy array ($H\times W\times C$), torch tensor ($C\times H\times W$)) and `mask`.
    - Return torch tensors ($1\times C\times H\times W$) masked_image, inpainted_image, image
"""
# Commented out IPython magic to ensure Python compatibility.
# %pip install lightning

# Commented out IPython magic to ensure Python compatibility.
# %pip install kornia

from torchvision import transforms, models
from torch.utils.data import Dataset, DataLoader
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import AdamW

import pytorch_lightning as pl
from pytorch_lightning.callbacks import Callback, ModelCheckpoint, EarlyStopping
from pytorch_lightning.loggers import TensorBoardLogger

import numpy as np
from PIL import Image
import os
import random
from tqdm import tqdm
import shutil
import matplotlib.pyplot as plt
from IPython import display

import time


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
image_folder='/content/images/images'

# """## Place2 dataset"""

# !gdown "https://drive.google.com/uc?export=download&id=16GYHdSWS3iMYwMPv5FpeDZN2rH7PR0F2"
# !tar -xzf data.tar.gz
# source_root = "/content/images/train"
# target_folder = "/content/images/images"

# os.makedirs(target_folder, exist_ok=True)

# for root, dirs, files in tqdm(os.walk(source_root), desc="Downloading..."):
#     for file in files:
#         if file.lower().endswith((".jpg", ".jpeg", ".png", ".gif", ".bmp", ".tiff")):
#             src = os.path.join(root, file)
#             dst = os.path.join(target_folder, file)

#             if os.path.exists(dst):
#                 base, ext = os.path.splitext(file)
#                 i = 1
#                 while os.path.exists(dst):
#                     dst = os.path.join(target_folder, f"{base}_{i}{ext}")
#                     i += 1

#             shutil.copy(src, dst)

"""## COCO dataset"""

# !wget http://images.cocodataset.org/zips/val2017.zip

"""# Utils"""

def plot_show(train_hist, valid_hist):
    plt.figure(figsize=(10, 5))
    plt.plot(train_hist, label='Train Loss')
    plt.plot(valid_hist, label='Valid Loss')
    plt.legend()
    plt.show()

def show_images(masked_img, output, original_img, n=1):
    masked_img = masked_img.cpu()
    output = output.cpu()
    original_img = original_img.cpu()

    masked_img_display = masked_img[:n]
    output_display = output[:n]
    original_img_display = original_img[:n]

    combined_output = output_display.clone()
    combined_output[masked_img_display != 0] = original_img_display[masked_img_display != 0]

    fig, axs = plt.subplots(n, 3, figsize=(12, 4 * n))

    if n == 1:
        axs = np.array([axs])

    for i in range(n):
        axs[i, 0].imshow(np.transpose(masked_img_display[i].numpy(), (1, 2, 0)))
        axs[i, 0].set_title("Masked Input")
        axs[i, 0].axis('off')

        axs[i, 1].imshow(np.transpose(combined_output[i].detach().numpy(), (1, 2, 0)))
        axs[i, 1].set_title("Inpainted Output")
        axs[i, 1].axis('off')

        axs[i, 2].imshow(np.transpose(original_img_display[i].numpy(), (1, 2, 0)))
        axs[i, 2].set_title("Original")
        axs[i, 2].axis('off')

    plt.tight_layout()
    plt.show()

"""# Modules"""

class COCODataset(Dataset):
    def __init__(self, image_folder, transform=None, ds_start=0, ds_end=2000, mask_size=32):
        self.image_folder = image_folder
        self.image_paths = [os.path.join(image_folder, fname) for fname in os.listdir(image_folder)[ds_start:ds_end]]
        self.transform = transform
        self.mask_size = mask_size

    def __len__(self):
        return len(self.image_paths)

    def apply_random_mask(self, image):
        _, h, w = image.shape
        top = random.randint(0, h - self.mask_size)
        left = random.randint(0, w - self.mask_size)

        mask = torch.ones((1, h, w))
        rand_h = int(random.random() * self.mask_size * 0.5 + self.mask_size)
        rand_w = int(random.random() * self.mask_size * 0.5 + self.mask_size)
        rand_h = min(rand_h, h-top-1)
        rand_w = min(rand_w, w-left-1)
        mask[:, top:top+rand_h, left:left+rand_w] = 0
        masked_image = image * mask

        return masked_image, mask

    def apply_random_small_maskes(self, image):
        _, h, w = image.shape
        num_masks = random.randint(1, 10)
        mask = torch.ones((1, h, w))

        for _ in range(num_masks):
            top = random.randint(0, h - self.mask_size)
            left = random.randint(0, w - self.mask_size)
            mask[:, top:top+self.mask_size, left:left+self.mask_size] = 0

        masked_image = image * mask

        return masked_image, mask

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        image = Image.open(img_path).convert('RGB')
        if self.transform:
            image = self.transform(image)
        masked_image, mask = self.apply_random_mask(image)
        # masked_image, mask = self.apply_random_small_maskes(image)
        # return masked_image, mask, image  # input, target
        return {'image': image, 'mask': 1 - mask}

"""# LaMA pretrained"""

# !pip install simple-lama-inpainting

# masked_image, mask, image = train_ds[i]
# show_images(masked_image, result, image)

# %%time
# masked_images, results, images = [], [], []
# for i in range(len(train_ds)):
#     masked_image, mask, image = train_ds[i]
#     mask_inv = 1 - mask
#     result = simple_lama(
#         transforms.ToPILImage()(masked_image),
#         transforms.ToPILImage()(mask_inv).convert('L'))

#     masked_images.append(masked_image)
#     results.append(transforms.ToTensor()(result))
#     images.append(image)

# show_images(torch.stack(masked_images), torch.stack(results), torch.stack(images), n=len(train_ds))

"""# Lama retrain

## Fourier Unit
"""

class FourierUnit(nn.Module):

    def __init__(self, in_channels, out_channels, groups=1, spatial_scale_mode='bilinear', fft_norm='ortho'):
        super(FourierUnit, self).__init__()
        self.groups = groups

        self.conv_layer = nn.Conv2d(in_channels * 2, out_channels * 2,
                                    kernel_size=1, groups=self.groups, bias=False)
        self.relu = torch.nn.ReLU(inplace=True)

        self.spatial_scale_mode = spatial_scale_mode
        self.fft_norm = fft_norm

    def forward(self, x):
        batch = x.shape[0]

        r_size = x.size()
        # (batch, c, h, w/2+1, 2)
        fft_dim = (-2, -1)
        ffted = torch.fft.rfftn(x, dim=fft_dim, norm=self.fft_norm)
        ffted = torch.stack((ffted.real, ffted.imag), dim=-1)
        ffted = ffted.permute(0, 1, 4, 2, 3).contiguous()  # (batch, c, 2, h, w/2+1)
        ffted = ffted.view((batch, -1,) + ffted.size()[3:])

        ffted = self.conv_layer(ffted)  # (batch, c*2, h, w/2+1)
        ffted = self.relu(ffted)

        ffted = ffted.view((batch, -1, 2,) + ffted.size()[2:]).permute(
            0, 1, 3, 4, 2).contiguous()  # (batch,c, t, h, w/2+1, 2)
        ffted = torch.complex(ffted[..., 0], ffted[..., 1])

        ifft_shape_slice = x.shape[-2:]
        output = torch.fft.irfftn(ffted, s=ifft_shape_slice, dim=fft_dim, norm=self.fft_norm)

        return output

"""## Spectral Transform"""

class SpectralTransform(nn.Module):

    def __init__(self, in_channels, out_channels, stride=1, groups=1,
                 enable_lfu=True, **fu_kwargs):
        super(SpectralTransform, self).__init__()
        self.enable_lfu = enable_lfu
        if stride == 2:
            self.downsample = nn.AvgPool2d(kernel_size=(2, 2), stride=2)
        else:
            self.downsample = nn.Identity()

        self.stride = stride
        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels // 2,
                      kernel_size=1, groups=groups, bias=False),
            nn.BatchNorm2d(out_channels // 2),
            nn.ReLU(inplace=True)
        )
        self.fu = FourierUnit(
            out_channels // 2, out_channels // 2, groups, **fu_kwargs)

        if self.enable_lfu:
            self.lfu = FourierUnit(
                out_channels // 2, out_channels // 2, groups)
        self.conv2 = torch.nn.Conv2d(
            out_channels // 2, out_channels, kernel_size=1, groups=groups, bias=False)

    def forward(self, x):
        x = self.downsample(x)
        x = self.conv1(x)
        output = self.fu(x)

        # flu
        if self.enable_lfu:
            n, c, h, w = x.shape
            split_no = 2
            split_s = h // split_no
            xs = torch.cat(torch.split(
                x[:, :c // 4], split_s, dim=-2), dim=1).contiguous()
            xs = torch.cat(torch.split(xs, split_s, dim=-1),
                            dim=1).contiguous()
            xs = self.lfu(xs)
            xs = xs.repeat(1, 1, split_no, split_no).contiguous()
        else:
            xs = 0

        output = self.conv2(x + output + xs)

        return output

"""## FFC"""

class FFC(nn.Module):

    def __init__(self, in_channels, out_channels, kernel_size,
                 ratio_gin, ratio_gout, stride=1, padding=0,
                 dilation=1, groups=1, bias=False,
                 padding_type='reflect', **spectral_kwargs):
        super(FFC, self).__init__()

        self.stride = stride

        in_cg = int(in_channels * ratio_gin)
        in_cl = in_channels - in_cg
        out_cg = int(out_channels * ratio_gout)
        out_cl = out_channels - out_cg

        self.ratio_gin = ratio_gin
        self.ratio_gout = ratio_gout
        self.global_in_num = in_cg

        module = nn.Identity if in_cl == 0 or out_cl == 0 else nn.Conv2d
        self.convl2l = module(in_cl, out_cl, kernel_size,
                              stride, padding, dilation, groups, bias, padding_mode=padding_type)
        module = nn.Identity if in_cl == 0 or out_cg == 0 else nn.Conv2d
        self.convl2g = module(in_cl, out_cg, kernel_size,
                              stride, padding, dilation, groups, bias, padding_mode=padding_type)
        module = nn.Identity if in_cg == 0 or out_cl == 0 else nn.Conv2d
        self.convg2l = module(in_cg, out_cl, kernel_size,
                              stride, padding, dilation, groups, bias, padding_mode=padding_type)
        module = nn.Identity if in_cg == 0 or out_cg == 0 else SpectralTransform
        self.convg2g = module(in_cg, out_cg,
                                         stride, 1 if groups == 1 else groups // 2, **spectral_kwargs)

    def forward(self, x):
        x_l, x_g = x if type(x) is tuple else (x, 0)
        # print("FFC")
        out_xl, out_xg = 0, 0

        if self.ratio_gout != 1:
            out_xl = self.convl2l(x_l) + self.convg2l(x_g)
        if self.ratio_gout != 0:
            out_xg = self.convl2g(x_l) + self.convg2g(x_g)

        return out_xl, out_xg

"""## FFC_BN_ACT"""

class FFC_BN_ACT(nn.Module):

    def __init__(self, in_channels, out_channels,
                 kernel_size, ratio_gin, ratio_gout,
                 stride=1, padding=0, dilation=1, groups=1, bias=False,
                 norm_layer=nn.BatchNorm2d, activation_layer=nn.Identity,
                 padding_type='reflect', **kwargs):
        super(FFC_BN_ACT, self).__init__()
        self.ffc = FFC(in_channels, out_channels, kernel_size,
                       ratio_gin, ratio_gout, stride, padding, dilation,
                       groups, bias, padding_type=padding_type, **kwargs)
        lnorm = nn.Identity if ratio_gout == 1 else norm_layer
        gnorm = nn.Identity if ratio_gout == 0 else norm_layer
        global_channels = int(out_channels * ratio_gout)
        self.bn_l = lnorm(out_channels - global_channels)
        self.bn_g = gnorm(global_channels)

        lact = nn.Identity if ratio_gout == 1 else activation_layer
        gact = nn.Identity if ratio_gout == 0 else activation_layer
        self.act_l = lact(inplace=True)
        self.act_g = gact(inplace=True)

    def forward(self, x):
        # print("FFC_BN_ACT")
        x_l, x_g = self.ffc(x)
        x_l = self.act_l(self.bn_l(x_l))
        x_g = self.act_g(self.bn_g(x_g))
        return x_l, x_g

"""## FFCResNetBlock"""

class FFCResnetBlock(nn.Module):
    def __init__(self, dim, padding_type, norm_layer, activation_layer=nn.ReLU, dilation=1,
                 inline=False, **conv_kwargs):
        super().__init__()
        self.conv1 = FFC_BN_ACT(dim, dim, kernel_size=3, padding=dilation, dilation=dilation,
                                norm_layer=norm_layer,
                                activation_layer=nn.ReLU,
                                padding_type=padding_type,
                                **conv_kwargs)
        self.conv2 = FFC_BN_ACT(dim, dim, kernel_size=3, padding=dilation, dilation=dilation,
                                norm_layer=norm_layer,
                                activation_layer=nn.ReLU,
                                padding_type=padding_type,
                                **conv_kwargs)
        self.inline = inline

    def forward(self, x):
        if self.inline:
            x_l, x_g = x[:, :-self.conv1.ffc.global_in_num], x[:, -self.conv1.ffc.global_in_num:]
        else:
            x_l, x_g = x if type(x) is tuple else (x, 0)
        # print("FFCResNetBlock")
        # print("x_l", x_l.shape)
        # print("x_g", x_g.shape)

        id_l, id_g = x_l, x_g

        x_l, x_g = self.conv1((x_l, x_g))
        x_l, x_g = self.conv2((x_l, x_g))

        x_l, x_g = id_l + x_l, id_g + x_g
        out = x_l, x_g
        if self.inline:
            out = torch.cat(out, dim=1)
        return out

"""## ConcatTupleLayer"""

class ConcatTupleLayer(nn.Module):
    def forward(self, x):
        assert isinstance(x, tuple)
        x_l, x_g = x
        assert torch.is_tensor(x_l) or torch.is_tensor(x_g)
        if not torch.is_tensor(x_g):
            return x_l
        return torch.cat(x, dim=1)

"""## FFCResNetGenerator"""

class FFCResNetGenerator(nn.Module):
    def __init__(self, input_nc=4, output_nc=3, ngf=64, n_downsampling=3, n_blocks=6,
                 norm_layer=nn.BatchNorm2d,
                 padding_type='reflect', activation_layer=nn.ReLU,
                 up_norm_layer=nn.BatchNorm2d, up_activation=nn.ReLU(True),
                 init_conv_kwargs={"ratio_gin": 0, "ratio_gout": 0},
                 downsample_conv_kwargs={"ratio_gin": 0, "ratio_gout": 0},
                 resnet_conv_kwargs={"ratio_gin": 0.75, "ratio_gout": 0.75}):
        super().__init__()

        model = [nn.ReflectionPad2d(3),
                 FFC_BN_ACT(input_nc, ngf, kernel_size=7, padding=0, norm_layer=norm_layer,
                            activation_layer=activation_layer, **init_conv_kwargs)]

        # downsample
        for i in range(n_downsampling):
            mult = 2 ** i
            if i == n_downsampling - 1:
                cur_conv_kwargs = dict(downsample_conv_kwargs)
                cur_conv_kwargs['ratio_gout'] = resnet_conv_kwargs.get('ratio_gin', 0)
            else:
                cur_conv_kwargs = downsample_conv_kwargs
            model += [FFC_BN_ACT(ngf * mult,
                                 ngf * mult * 2,
                                 kernel_size=3, stride=2, padding=1,
                                 norm_layer=norm_layer,
                                 activation_layer=activation_layer,
                                 **cur_conv_kwargs)]

        mult = 2 ** n_downsampling
        feats_num_bottleneck = ngf * mult

        # resnet blocks
        for i in range(n_blocks):
            cur_resblock = FFCResnetBlock(feats_num_bottleneck, padding_type=padding_type,
                                          activation_layer=activation_layer,
                                          norm_layer=norm_layer, **resnet_conv_kwargs)
            model += [cur_resblock]

        model += [ConcatTupleLayer()]

        # upsample
        for i in range(n_downsampling):
            mult = 2 ** (n_downsampling - i)
            model += [nn.ConvTranspose2d(ngf * mult,
                                         ngf * mult // 2,
                                         kernel_size=3, stride=2, padding=1, output_padding=1),
                      up_norm_layer(ngf * mult // 2),
                      up_activation]

        model += [nn.ReflectionPad2d(3),
                  nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]

        model.append(nn.Sigmoid())

        self.model = nn.Sequential(*model)

    def forward(self, input):
        return self.model(input)

"""## FFCResNetDiscriminator"""

class FFCNLayerDiscriminator(nn.Module):
    def __init__(self, input_nc=3, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, max_features=512,
                 init_conv_kwargs={"ratio_gin": 0, "ratio_gout": 0},
                 conv_kwargs={"ratio_gin": 0, "ratio_gout": 0}):
        super().__init__()
        self.n_layers = n_layers

        def _act_ctor(inplace=True):
            return nn.LeakyReLU(negative_slope=0.2, inplace=inplace)

        kw = 3
        padw = int(np.ceil((kw-1.0)/2))
        sequence = [[FFC_BN_ACT(input_nc, ndf, kernel_size=kw, padding=padw, norm_layer=norm_layer,
                                activation_layer=_act_ctor, **init_conv_kwargs)]]

        nf = ndf
        for n in range(1, n_layers):
            nf_prev = nf
            nf = min(nf * 2, max_features)

            cur_model = [
                FFC_BN_ACT(nf_prev, nf,
                           kernel_size=kw, stride=2, padding=padw,
                           norm_layer=norm_layer,
                           activation_layer=_act_ctor,
                           **conv_kwargs)
            ]
            sequence.append(cur_model)

        nf_prev = nf
        nf = min(nf * 2, 512)

        cur_model = [
            FFC_BN_ACT(nf_prev, nf,
                       kernel_size=kw, stride=1, padding=padw,
                       norm_layer=norm_layer,
                       activation_layer=lambda *args, **kwargs: nn.LeakyReLU(*args, negative_slope=0.2, **kwargs),
                       **conv_kwargs),
            ConcatTupleLayer()
        ]
        sequence.append(cur_model)

        sequence += [[nn.Conv2d(nf, 1, kernel_size=kw, stride=1, padding=padw)]]

        for n in range(len(sequence)):
            setattr(self, 'model'+str(n), nn.Sequential(*sequence[n]))

    def get_all_activations(self, x):
        res = [x]
        for n in range(self.n_layers + 2):
            model = getattr(self, 'model' + str(n))
            res.append(model(res[-1]))
        return res[1:]

    def forward(self, x):
        act = self.get_all_activations(x)
        feats = []
        for out in act[:-1]:
            if isinstance(out, tuple):
                if torch.is_tensor(out[1]):
                    out = torch.cat(out, dim=1)
                else:
                    out = out[0]
            feats.append(out)
        return act[-1], feats

"""## FakeFakes"""

import torch
from kornia.constants import SamplePadding
from kornia.augmentation import RandomAffine, CenterCrop


class FakeFakesGenerator:
    def __init__(self, aug_proba=0.5, img_aug_degree=30, img_aug_translate=0.2):
        self.grad_aug = RandomAffine(degrees=360,
                                     translate=0.2,
                                     padding_mode=SamplePadding.REFLECTION,
                                     keepdim=False,
                                     p=1)
        self.img_aug = RandomAffine(degrees=img_aug_degree,
                                    translate=img_aug_translate,
                                    padding_mode=SamplePadding.REFLECTION,
                                    keepdim=True,
                                    p=1)
        self.aug_proba = aug_proba

    def __call__(self, input_images, masks):
        blend_masks = self._fill_masks_with_gradient(masks)
        blend_target = self._make_blend_target(input_images)
        result = input_images * (1 - blend_masks) + blend_target * blend_masks
        return result, blend_masks

    def _make_blend_target(self, input_images):
        batch_size = input_images.shape[0]
        permuted = input_images[torch.randperm(batch_size)]
        augmented = self.img_aug(input_images)
        is_aug = (torch.rand(batch_size, device=input_images.device)[:, None, None, None] < self.aug_proba).float()
        result = augmented * is_aug + permuted * (1 - is_aug)
        return result

    def _fill_masks_with_gradient(self, masks):
        batch_size, _, height, width = masks.shape
        grad = torch.linspace(0, 1, steps=width * 2, device=masks.device, dtype=masks.dtype) \
            .view(1, 1, 1, -1).expand(batch_size, 1, height * 2, width * 2)
        grad = self.grad_aug(grad)
        grad = CenterCrop((height, width))(grad)
        grad *= masks

        grad_for_min = grad + (1 - masks) * 10
        grad -= grad_for_min.view(batch_size, -1).min(-1).values[:, None, None, None]
        grad /= grad.view(batch_size, -1).max(-1).values[:, None, None, None] + 1e-6
        grad.clamp_(min=0, max=1)

        return grad

"""## Losses"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models

class ResNetPL(nn.Module):

    def __init__(self):
        super().__init__()
        base_resnet = models.resnet50(pretrained=True)
        self.slice1 = nn.Sequential(*list(base_resnet.children())[:4]) # Conv1 -> MaxPool
        self.slice2 = base_resnet.layer1
        self.slice3 = base_resnet.layer2
        self.slice4 = base_resnet.layer3

        for param in self.parameters():
            param.requires_grad = False
        self.eval()

    def forward(self, x):
        # Normalize: [0, 1] -> ImageNet mean/std
        x = (x - torch.tensor([0.485, 0.456, 0.406]).to(x).view(1,3,1,1)) / \
            torch.tensor([0.229, 0.224, 0.225]).to(x).view(1,3,1,1)

        feat1 = self.slice1(x)
        feat2 = self.slice2(feat1)
        feat3 = self.slice3(feat2)
        feat4 = self.slice4(feat3)
        return [feat1, feat2, feat3, feat4]

class GeneratorLoss(nn.Module):

    def __init__(self, weight_l1=10.0, weight_adv=10.0, weight_fm=100.0, weight_pl=30.0):
        super().__init__()
        self.weight_l1 = weight_l1
        self.weight_adv = weight_adv
        self.weight_fm = weight_fm
        self.weight_pl = weight_pl

        self.resnet_pl = ResNetPL()
        self.l1_loss = nn.L1Loss(reduction='none')

    # |pred - target| of the area OUTSIDE of the hole
    def background_l1_loss(self, pred, target, mask):
        loss = self.l1_loss(pred, target)
        return ((1 - mask) * loss).sum() / ((1 - mask).sum() + 1e-8)

    # Check the MSE for the output (extracted feature) of each layer in Discriminator
    # in the area OUTSIDE of the hole
    def feature_matching_loss(self, fake_feats, real_feats, mask):
        loss = 0.0
        for f_fake, f_real in zip(fake_feats, real_feats):
            cur_mask = F.interpolate(mask, size=f_fake.shape[-2:], mode='bilinear')
            cur_loss = ((f_fake - f_real.detach())**2 * (1 - cur_mask)).sum()
            cur_loss /= ((1 - cur_mask).sum() + 1e-8)
            loss += cur_loss
        return loss / len(fake_feats)


    def adversarial_loss(self, disc_logits_fake):
        return F.softplus(-disc_logits_fake).mean()

    def perceptual_loss(self, pred, target):
        fake_feats = self.resnet_pl(pred)
        real_feats = self.resnet_pl(target)
        loss = 0.0
        for f_fake, f_real in zip(fake_feats, real_feats):
            loss += F.mse_loss(f_fake, f_real.detach())
        return loss

    def forward(self, pred_img, target_img, mask, disc_logits_fake, disc_feats_fake, disc_feats_real):
        losses = {}

        losses['l1'] = self.background_l1_loss(pred_img, target_img, mask) * self.weight_l1
        losses['adv'] = self.adversarial_loss(disc_logits_fake) * self.weight_adv
        losses['fm'] = self.feature_matching_loss(disc_feats_fake, disc_feats_real, mask) * self.weight_fm
        losses['pl'] = self.perceptual_loss(pred_img, target_img) * self.weight_pl

        total_loss = sum(losses.values())
        return total_loss, losses

class DiscriminatorLoss(nn.Module):

    def __init__(self, r1_coef=10):
        super().__init__()
        self.r1_coef = r1_coef

    def forward(self, discriminator, real_img, fake_img, mask):
        real_img.requires_grad_(True)
        fake_img = fake_img * mask + real_img * (1 - mask)

        disc_real_pred, _ = discriminator(real_img)
        disc_fake_pred, _ = discriminator(fake_img)
        mask_pred = F.interpolate(mask, size=disc_real_pred.shape[-2:], mode='bilinear')

        real_loss = F.softplus(-disc_real_pred).mean()
        fake_masked_loss = F.softplus(disc_fake_pred * mask_pred).mean()
        fake_unmasked_loss = F.softplus(-disc_fake_pred * (1 - mask_pred)).mean()

        loss = real_loss + fake_masked_loss + fake_unmasked_loss

        grad_real = torch.autograd.grad(
            outputs=disc_real_pred.sum(),
            inputs=real_img,
            create_graph=True
        )[0]

        grad_penalty = (grad_real.view(grad_real.shape[0], -1).norm(2, dim=1) ** 2).mean()
        loss += self.r1_coef * grad_penalty

        return loss

"""## Previous version"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models

class ResNetPL(nn.Module):
    def __init__(self, weights=None):
        super().__init__()
        base_resnet = models.resnet50(pretrained=True)
        self.slice1 = nn.Sequential(*list(base_resnet.children())[:4]) # Conv1 -> MaxPool
        self.slice2 = base_resnet.layer1
        self.slice3 = base_resnet.layer2
        self.slice4 = base_resnet.layer3

        for param in self.parameters():
            param.requires_grad = False
        self.eval()

    def forward(self, x):
        # Normalize: [0, 1] -> ImageNet mean/std
        x = (x - torch.tensor([0.485, 0.456, 0.406]).to(x).view(1,3,1,1)) / \
            torch.tensor([0.229, 0.224, 0.225]).to(x).view(1,3,1,1)

        feat1 = self.slice1(x)
        feat2 = self.slice2(feat1)
        feat3 = self.slice3(feat2)
        feat4 = self.slice4(feat3)
        return [feat1, feat2, feat3, feat4]

class GeneratorLoss(nn.Module):
    def __init__(self, weight_l1=10.0, weight_adv=10.0, weight_fm=100.0, weight_pl=30.0,
                 resnet_weights=None):
        super().__init__()
        self.weight_l1 = weight_l1
        self.weight_adv = weight_adv
        self.weight_fm = weight_fm
        self.weight_pl = weight_pl

        self.resnet_pl = ResNetPL(weights=resnet_weights)
        self.l1_loss = nn.L1Loss(reduction='none')

    def masked_l1_loss(self, pred, target, mask):
        loss = self.l1_loss(pred, target)
        return loss.mean()

    def feature_matching_loss(self, fake_feats, real_feats):
        loss = 0.0
        for f_fake, f_real in zip(fake_feats, real_feats):
            loss += F.l1_loss(f_fake, f_real.detach())
        return loss

    def adversarial_loss(self, discriminator_logits_fake):
        return F.softplus(-discriminator_logits_fake).mean()

    def perceptual_loss(self, pred, target):
        fake_feats = self.resnet_pl(pred)
        real_feats = self.resnet_pl(target)
        loss = 0.0
        for f_fake, f_real in zip(fake_feats, real_feats):
            loss += F.mse_loss(f_fake, f_real.detach())
        return loss / len(fake_feats)

    def forward(self, pred_img, target_img, mask, disc_logits_fake, disc_feats_fake, disc_feats_real):
        losses = {}

        losses['l1'] = self.masked_l1_loss(pred_img, target_img, mask) * self.weight_l1
        losses['adv'] = self.adversarial_loss(disc_logits_fake) * self.weight_adv
        losses['fm'] = self.feature_matching_loss(disc_feats_fake, disc_feats_real) * self.weight_fm
        losses['pl'] = self.perceptual_loss(pred_img, target_img) * self.weight_pl

        total_loss = sum(losses.values())
        return total_loss, losses

class DiscriminatorLoss(nn.Module):

    def __init__(self, r1_coef=1e-3):
        super().__init__()
        self.r1_coef = r1_coef

    def forward(self, discriminator, real_img, fake_img, mask):
        real_img.requires_grad_(True)
        fake_img = fake_img * mask + real_img * (1 - mask)

        disc_real_pred, _ = discriminator(real_img)
        disc_fake_pred, _ = discriminator(fake_img)
        mask_pred = F.interpolate(mask, size=disc_real_pred.shape[-2:], mode='bilinear')

        real_loss = F.softplus(-disc_real_pred).mean()
        fake_masked_loss = F.softplus(disc_fake_pred * mask_pred).mean()
        fake_unmasked_loss = F.softplus(-disc_fake_pred * (1 - mask_pred)).mean()

        loss = real_loss + fake_masked_loss + fake_unmasked_loss

        grad_real = torch.autograd.grad(
            outputs=disc_real_pred.sum(),
            inputs=real_img,
            create_graph=True
        )[0]

        grad_penalty = (grad_real.view(grad_real.shape[0], -1).norm(2, dim=1) ** 2).mean()
        loss += self.r1_coef * grad_penalty

        return loss

"""- Original: Able to capture edge, shape. Blurry. Unmatched color.

## LamaTrainer
"""

import torch
import pytorch_lightning as pl
from torch.optim import AdamW

class LaMaTrainer(pl.LightningModule):
    def __init__(self, generator, discriminator, lr_g=1e-3, lr_d=1e-4,
                 fake_fakes_gen=None, fake_fakes_proba=0.2):
        super().__init__()

        # Lightning stuff
        self.save_hyperparameters(ignore=['generator', 'discriminator', 'fake_fakes_gen'])
        # ---------------

        self.generator = generator
        self.discriminator = discriminator
        self.fake_fakes_gen = fake_fakes_gen
        self.fake_fakes_proba = fake_fakes_proba
        self.lr_g = lr_g
        self.lr_d = lr_d

        self.loss_G_fn = GeneratorLoss()
        self.loss_D_fn = DiscriminatorLoss()

        # Lightning stuff
        self.automatic_optimization = False
        # ---------------

    def configure_optimizers(self):
        opt_g = AdamW(self.generator.parameters(), lr=self.lr_g, betas=(0.5, 0.9))
        opt_d = AdamW(self.discriminator.parameters(), lr=self.lr_d, betas=(0.5, 0.9))
        return [opt_g, opt_d], []

    def forward(self, batch):
        img = batch['image']
        mask = batch['mask']

        masked_img = img * (1 - mask)

        masked_img = torch.cat([masked_img, mask], dim=1)

        batch['predicted_image'] = self.generator(masked_img)

        batch['inpainted'] = mask * batch['predicted_image'] + (1 - mask) * batch['image']

        if self.training and self.fake_fakes_gen is not None and torch.rand(1).item() < self.fake_fakes_proba:
            batch['fake_fakes'], batch['fake_fakes_masks'] = self.fake_fakes_gen(img, mask)
            batch['use_fake_fakes'] = True
        else:
            batch['fake_fakes'] = torch.zeros_like(img)
            batch['fake_fakes_masks'] = torch.zeros_like(mask)
            batch['use_fake_fakes'] = False

        return batch

    # Lightning stuff
    def toggle_grads(self, model, requires_grad):
        for p in model.parameters():
            p.requires_grad = requires_grad
    # ---------------

    def training_step(self, batch, batch_idx):
        opt_g, opt_d = self.optimizers()

        # Run Forward Pass (generates fake_img etc.)
        batch = self(batch)

        real_img = batch['image']
        mask = batch['mask']
        fake_img = batch['predicted_image'] # Generator output

        # -------------------------------------------------
        #  1. Train Generator
        # -------------------------------------------------

        # Lightning stuff
        self.toggle_optimizer(opt_g)
        self.toggle_grads(self.generator, True)

        self.toggle_grads(self.discriminator, True)
        disc_logits_fake, disc_feats_fake = self.discriminator(fake_img)
        self.toggle_grads(self.discriminator, False)
        # ---------------

        with torch.no_grad():
            disc_logits_real, disc_feats_real = self.discriminator(real_img)

        loss_g, metrics = self.loss_G_fn(
            pred_img=fake_img,
            target_img=real_img,
            mask=mask,
            disc_logits_fake=disc_logits_fake,
            disc_feats_fake=disc_feats_fake,
            disc_feats_real=disc_feats_real
        )

        self.manual_backward(loss_g)
        opt_g.step()
        opt_g.zero_grad()
        self.untoggle_optimizer(opt_g)

        self.log_dict({f"train/gen_{k}": v for k, v in metrics.items()}, prog_bar=True)

        # -------------------------------------------------
        #  2. Train Discriminator
        # -------------------------------------------------
        self.toggle_optimizer(opt_d)
        self.toggle_grads(self.discriminator, True)
        self.toggle_grads(self.generator, False)

        if batch['use_fake_fakes']:
            fake_for_d = batch['fake_fakes'].detach()
        else:
            fake_for_d = fake_img.detach()

        loss_d = self.loss_D_fn(
            discriminator=self.discriminator,
            real_img=real_img,
            fake_img=fake_for_d,
            mask=mask
        )

        # Lightning stuff
        self.manual_backward(loss_d)
        opt_d.step()
        opt_d.zero_grad()
        self.untoggle_optimizer(opt_d)
        # ---------------

        self.log("train/disc_loss", loss_d, prog_bar=True)

    def validation_step(self, batch, batch_idx):
        batch = self(batch)

        real_img = batch['image']
        mask = batch['mask']
        fake_img = batch['predicted_image']

        disc_logits_fake, disc_feats_fake = self.discriminator(fake_img)
        disc_logits_real, disc_feats_real = self.discriminator(real_img)

        loss_g, metrics = self.loss_G_fn(
            pred_img=fake_img,
            target_img=real_img,
            mask=mask,
            disc_logits_fake=disc_logits_fake,
            disc_feats_fake=disc_feats_fake,
            disc_feats_real=disc_feats_real
        )
        self.log(f"val/gen_loss", loss_g, prog_bar=True)

        fake_for_d = fake_img.detach()

        with torch.enable_grad():
            real_img_for_d = real_img.detach().clone()
            real_img_for_d.requires_grad = True

            loss_d = self.loss_D_fn(
                discriminator=self.discriminator,
                real_img=real_img_for_d,
                fake_img=fake_for_d,
                mask=mask
            )
            real_img_for_d.requires_grad = False

        self.log("val/disc_loss", loss_d, prog_bar=True)

import torch
import pytorch_lightning as pl
import matplotlib.pyplot as plt
import numpy as np
from pytorch_lightning.callbacks import Callback

class PlottingCallback(Callback):
    """
    Plots images (Ground Truth, Masked, Inpainted) using Matplotlib
    every N epochs instead of saving to a folder.
    """
    def __init__(self, log_every_n_epochs=1):
        super().__init__()
        self.log_every_n_epochs = log_every_n_epochs

    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, n=3):
        if batch_idx != 0:
            return
        if (trainer.current_epoch + 1) % self.log_every_n_epochs != 0:
            return

        pl_module.eval()
        with torch.no_grad():
            batch = pl_module(batch)

        images = []
        masked_images = []
        inpainted_images = []
        for i in range(n):
            image = batch['image'][i].cpu()
            mask = batch['mask'][i].cpu()
            inpainted = batch['inpainted'][i].cpu()
            masked_image = image * (1 - mask)

            images.append(image)
            masked_images.append(masked_image)
            inpainted_images.append(inpainted)

        images = torch.stack(images)
        masked_images = torch.stack(masked_images)
        inpainted_images = torch.stack(inpainted_images)

        show_images(masked_images, inpainted_images, images, n=len(images))

        # img = batch['image'][0].cpu().clamp(0, 1)
        # mask = batch['mask'][0].cpu().clamp(0, 1)
        # inpainted = batch['inpainted'][0].cpu().clamp(0, 1)
        # masked_img = img * (1 - mask)

        # real_np = img.permute(1, 2, 0).numpy()
        # masked_np = masked_img.permute(1, 2, 0).numpy()
        # inpainted_np = inpainted.permute(1, 2, 0).numpy()

        # fig, axes = plt.subplots(1, 3, figsize=(15, 5))

        # axes[0].imshow(real_np)
        # axes[0].set_title("Ground Truth")
        # axes[0].axis('off')

        # axes[1].imshow(masked_np)
        # axes[1].set_title("Masked Input")
        # axes[1].axis('off')

        # axes[2].imshow(inpainted_np)
        # axes[2].set_title("Inpainted Output")
        # axes[2].axis('off')

        # plt.tight_layout()

        if trainer.logger and hasattr(trainer.logger.experiment, "add_figure"):
            trainer.logger.experiment.add_figure(
                "Validation/Comparison", fig, global_step=trainer.global_step
            )
            plt.close(fig)
        else:
            plt.show()

class ColabPlottingCallback(Callback):
    def __init__(self, log_every_n_epochs=5, save_dir="training_samples"):
        super().__init__()
        self.log_every_n_epochs = log_every_n_epochs
        self.save_dir = save_dir
        os.makedirs(self.save_dir, exist_ok=True)

    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
        if batch_idx != 0:
            return

        current_epoch = trainer.current_epoch
        if (current_epoch + 1) % self.log_every_n_epochs != 0 and current_epoch != 0:
            return

        pl_module.eval()
        with torch.no_grad():
            batch = pl_module(batch)

        img = batch['image'][0].detach().cpu().clamp(0, 1).permute(1, 2, 0).numpy()
        mask = batch['mask'][0].detach().cpu().clamp(0, 1).permute(1, 2, 0).numpy()
        inpainted = batch['inpainted'][0].detach().cpu().clamp(0, 1).permute(1, 2, 0).numpy()
        masked_img = img * (1 - mask)

        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        titles = ["Ground Truth", "Masked Input", "Inpainted Output"]
        images = [img, masked_img, inpainted]

        for ax, im, title in zip(axes, images, titles):
            ax.imshow(im)
            ax.set_title(title)
            ax.axis('off')

        plt.tight_layout()

        save_path = os.path.join(self.save_dir, f"epoch_{current_epoch}.png")
        plt.savefig(save_path)
        plt.close(fig)

        print(f"Epoch {current_epoch} Results:")
        display.display(display.Image(filename=save_path))

generator = FFCResNetGenerator()
discriminator = FFCNLayerDiscriminator()
model = LaMaTrainer(generator, discriminator)

checkpoint_callback = ModelCheckpoint(
    dirpath="checkpoints",
    filename="lama-{epoch:02d}-{val/gen_loss:.4f}",
    save_top_k=1,
    monitor="val/gen_loss",
    mode="min",
    save_last=True
)

early_stop_callback = EarlyStopping(
    monitor="val/gen_loss",
    min_delta=1.00,
    patience=5,
    verbose=True,
    mode="min"
)

plotting_callback = ColabPlottingCallback(log_every_n_epochs=1)

logger = TensorBoardLogger(
    save_dir="lightning_logs",
    name="",
    version=0
)

torch.set_float32_matmul_precision('high')

trainer = pl.Trainer(
    accelerator="gpu" if torch.cuda.is_available() else "cpu",
    devices=1,
    max_epochs=50,
    enable_progress_bar=True,
    logger=logger,
    callbacks=[
        plotting_callback,
        checkpoint_callback,
        early_stop_callback
    ]
)

# Setup
# transform = transforms.Compose([
#     transforms.ToTensor(),
#     transforms.Resize((128, 128))
# ])

# test_size = len(os.listdir(image_folder))
# valid_size = int(test_size * 0.8)
# train_size = int(test_size * 0.5)
# train_ds = COCODataset(image_folder, transform=transforms.ToTensor(), ds_start=0    , ds_end=train_size, mask_size=32)
# valid_ds = COCODataset(image_folder, transform=transforms.ToTensor(), ds_start=train_size , ds_end=valid_size, mask_size=32)
# train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=os.cpu_count())
# valid_dl = DataLoader(valid_ds, batch_size=32, shuffle=False, num_workers=os.cpu_count())

"""# Training process"""

# ckpt_path = "model.ckpt"
# trainer.fit(model, train_dl, valid_dl, ckpt_path=ckpt_path)
# trainer.fit(model, train_dl, valid_dl)

# img = torch.randn(1, 3, 128, 128)
# mask = torch.randn(1, 1, 128, 128)
# generator(torch.cat([img, mask], dim=1)).shape

"""# Evaluation"""

# from google.colab import files
# files.upload()

# transform = transforms.Compose([transforms.ToTensor()])
# train_ds = COCODataset(image_folder, transform=transform, ds_start=0    , ds_end=5, mask_size=32)
# image, mask = train_ds[0].values()
# image = image.unsqueeze(0)
# mask = mask.unsqueeze(0)
# masked_image = image * mask
# # Corrected: Repeat the single mask channel 3 times to match image channels, without changing height
# show_images(masked_image, mask.repeat(1, 3, 1, 1), mask.repeat(1, 3, 1, 1))

# def run_inference_from_folder(image_folder_path, ckpt_path="model.ckpt", device='cpu', n_records=5):
#     """
#     Expected to have an image folder.
#     """

#     print(f"Loading model from {ckpt_path}...")

#     generator = FFCResNetGenerator()
#     discriminator = FFCNLayerDiscriminator()

#     model = LaMaTrainer.load_from_checkpoint(
#         checkpoint_path=ckpt_path,
#         generator=generator,
#         discriminator=discriminator
#     )

#     model.to(device)
#     model.eval()
#     model.freeze()

#     transform = transforms.Compose([transforms.ToTensor()])
#     train_ds = COCODataset(
#         image_folder_path,
#         transform=transform,
#         ds_start=0,
#         ds_end=min(n_records, len(os.listdir(image_folder_path))),
#         mask_size=32
#     )

#     images = []
#     masked_images = []
#     inpainted_images = []
#     print("Inpainting...")
#     for i in range(len(train_ds)):
#         image, mask = train_ds[i].values()
#         images.append(image)
#         masked_image = image * (1 - mask)
#         # mask = 1 - mask
#         masked_images.append(masked_image)

#         image = image.unsqueeze(0).to(device)
#         mask = mask.unsqueeze(0).to(device)
#         res = model({
#             "image": image,
#             "mask": mask
#         })
#         inpainted_images.append(res['inpainted'].squeeze(0))

#     images = torch.stack(images)
#     masked_images = torch.stack(masked_images)
#     inpainted_images = torch.stack(inpainted_images)

#     show_images(masked_images, inpainted_images, images, n=len(images))

#     return masked_images, inpainted_images, images

# run_inference_from_folder(image_folder_path="images/test")

def run_inference_from_image(image, mask, ckpt_path="model.ckpt", device='cpu'):
    """
    Suitable for implementation.
    image: can be a path to img, `torch.Tensor` of `np.ndarray`
    """
    print("Running inference")
    start_time = time.time()

    def cvt2Tensor(img, c=3):
        """
        Convert mask (c=1) and image to torch.tensor
        NOTE: np.ndarray has shape (H, W, C) -> torch.tensor (C, H, W)
        """

        transform = transforms.ToTensor()
        if (isinstance(img, str)):
            img = transform(Image.open(img))
        if (isinstance(img, np.ndarray) or isinstance(img, Image.Image)):
            img = transform(img)
        return img[:c, ...].unsqueeze(0).to(device)

    # Model init
    print(f"Loading model from {ckpt_path}...")

    generator = FFCResNetGenerator()
    discriminator = FFCNLayerDiscriminator()

    model = LaMaTrainer.load_from_checkpoint(
        checkpoint_path=ckpt_path,
        generator=generator,
        discriminator=discriminator
    )

    model.to(device)
    model.eval()
    model.freeze()

    # Image init
    image = cvt2Tensor(image)
    mask = cvt2Tensor(mask, c=1)

    masked_image = image * (1 - mask)
    mask = 1 - mask

    res = model({
        "image": image,
        "mask": mask
    })
    inpainted_image = res['inpainted']

    print(f"Running inference completed in {time.time() - start_time:.5f}s")

    show_images(masked_image, inpainted_image, image, n=1)

    return masked_image, inpainted_image, image

if __name__ == "__main__":
    from images import getImageFromUrl
    from coco_data import getRandomUrl

    srcpath = os.path.dirname(__file__)
    apppath = os.path.dirname(srcpath)
    assetspath = os.path.join(apppath, 'assets')
    ckpt_path = os.path.join(assetspath, "lightning_logs", "model.ckpt")

    print("Rolling the dice for a good image:")
    start_time = time.time()
    img = getImageFromUrl(getRandomUrl(samesize=True))
    img = img.resize((128, 128))
    print(f"Rolling complete in {time.time() - start_time:.5f}s; image size: {img.width}, {img.height}")

    print("Creating mask:")
    start_time = time.time()
    mask_dummy = torch.zeros(9, img.height, img.width)
    mask_size = 16 # /shrug
    center_x, center_y = img.width // 2, img.height // 2
    start_x, end_x = center_x - mask_size // 2, center_x + mask_size // 2
    start_y, end_y = center_y - mask_size // 2, center_y + mask_size // 2
    mask_dummy[:, start_y:end_y, start_x:end_x] = 1
    print(f"Created mask in {time.time() - start_time:.5f}s; mask size: {mask_size}, top left coords:() {start_x},{start_y})")

    run_inference_from_image(image=img, mask=mask_dummy, ckpt_path=ckpt_path)
